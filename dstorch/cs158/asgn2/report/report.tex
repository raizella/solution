\documentclass[11pt]{article}

\usepackage{fullpage}

\parskip 7pt
\parindent 0pt

\title{Project Part 2}
\author{David Storch and Matt Mahoney \\
	logins: dstorch, mjmahone \\
	Team: Pugnacious Parsers}
\date{4 March 2011}

\begin{document}

\maketitle

\section{Question 1}

The overarching strategy in creating our inverted index was to reduce the
processing required at query time for top $K$ retrieval. To this end, we pre-computed
the inverse document frequency $idf_{t}$ for each term and the Euclidean normalization $\sqrt{\sum_{t'\in T_{d}} tf^{2}_{t', d}}$ for each document.

Our inverted index file first contains a term-position pair
for each term in the lexicon (written one per line to the beginning of the file), where the ``position'' is the number of the byte in the index at which the
corresponding postings list begins---allowing us to read postings lists from the disk only as they
are needed. The term-position pairs must be read into into memory at query time in order to build the b-tree.
Next, the index contains docID-Euclidian normalization factor pairs written one per line.

Lastly, the index contains a postings list for each term of the form:
\begin{verbatim}
<idf of term> : <docID> <position> <position> ... : <docID> <position> <position> ...
\end{verbatim}

Our inverted index is 245,866,104 bytes, which is 72.4\% the size of the original collection.

\section{Question 2}

Our createIndex program's time complexity is $O(n)$, where $n$ is the size of the original collection file (i.e.,
the total number of words in the entire collection, counting repititions).
It makes one linear pass to parse the XML, another pass to construct the inverted index (i.e., each postings list), 
another pass to compute the Euclidean normalization factors, and a final pass to write to the output file. The space complexity
is also $O(n)$. Under the assumption that the number of terms $t$ in the lexicon and the number of documents
$d$ in the collection satisfy $t << n$ and $d << n$, we can ignore everything in the index file other than
the postings lists. We get $O(n)$ because there is one positional entry for word in the original file; the docIDs in the postings
lists do not contribute to the space complexity because the number of docIDs which we write out is less than or equal to $n$.
We can ignore the $idf_{t}$ values, because there are $O(t)$ such values and $t << n$.


\section{Question 3}

When \texttt{queryIndex} is invoked, it reads from the beginning of the inverted index file in order
to construct the b-tree of query terms, and to load the normalization factors for each docID.
For a given query with a set $Q$ of query terms, we read a single postings list from the inverted
index for each $q \in Q$. For example, processing the free text query \texttt{gargantuan proportions}
requires us to read only two postings lists from the index file. These postings lists are cached and used throughout the remainder of the
querying and ranking process in order to avoid repeated disk reads.

\section{Question 4}

\subsection{Permuterm b-tree}

We use a b-tree to store all the permuterms of every term in the lexicon---\texttt{tree} is stored four times in the
b-tree as \texttt{\$tree}, \texttt{t\$ree}, \texttt{tr\$ee}, and \texttt{tre\$e}.

\subsection{Answering wildcard queries}

We decompose the wildcard query, which may
have an arbitrary number of wildcards, into a series of single wildcard queries. To answer a single
wildcard query, the asterisk is rotated to the end of the word, which gives us the range of keys from the permuterm b-tree.

\subsection{Wildcard query time complexity}

Extracting the range from the b-tree requires us to locate a subtree, and then iterate on the elements of that subtree. The cost of locating the subtree is bounded
by the depth of the b-tree, which is $O(\log t)$ if $t$ is the number of terms in the lexicon. Iterating
on this subtree is linear in $m$, the number of matching b-tree terms which belong
in the range (i.e. the size of the subtree). Hence the cost of extracting the range is $O(m + \log t)$.

For each of the $m$ matches in the range, we must perform a one-word query. Let the cost of a one-word query
be $k$. In order to answer a one-word query, we just read the corresponding postings list from the disk---however,
the cost of the disk read operation increases with posting list length, so $k$ depends on the frequency of the term in the
lexicon. The total time complexity for a single wildcard query is $O(km + \log t)$.

A double wildcard query is decomposed into two single wildcard queries, plus the cost of intersecting the
set of matching terms $M_{1}$ and $M_{2}$ for the two single wildcard, where the cardinality of $M_{1}$ is $m_{1}$ and
the cardinality of $M_{2}$ is $m_{2}$. Using hash sets, finding $M_{1} \cap M_{2}$ should take $O(m_{1} + m_{2})$ time,
giving the total complexity $O(k(m_{1} + m_{2}) + \log t)$.

\section{Question 5}

\subsection{One-word queries}

The query \texttt{tree} provides an example of an input for which the top $K$ ranking is not
particularly good. All of the results are related to trees, but the article entitled ``Tree'' is
the third hit, and some more obscure articles made their way into the top 10, including ``Unrooted
binary tree'' as the 10th hit. The term ``tree'' appears extremely frequently in the collection, which
means that there are lots of relevant hits, and our algorithm isn't subtle enough to determine which
is the \textit{most} relevant.

The query \texttt{unrooted}, on the other hand, produced much better results. The first hit was
``Unrooted binary tree'', and subsequent hits described types of unrooted trees (such as phylogenetic
trees). The term ``unrooted'' is more obscure, so it is more likely that the limited number of relevant
hits will be what the end-user is looking for.

\subsection{Free-text queries}

The query \texttt{red-black tree} produces as its first two hits the articles ``Red-black'' and
``Red-black tree''. The reason that ``Red-black'' is ranked higher than the article that the user
is probably looking for is because ``Red-black'' is a short stub article---the results are biased
by length normalization. Also, many of the hits are articles which contain ``tree'' but not
``red'' or ``black''. These articles repeat the term ``tree'' over and over again, which makes
them highly weighted, despite the fact that they don't contain all the query terms. The query
\texttt{red AND black AND tree} produces better results for this reason.

One free-text query which worked well was \texttt{programming languages}. The top articles
for this query were ``List of programming languages by category'',  ``List of object-oriented programming languages'',
and ``List of programming languages''. Note again that the short articles which are just lists are stubs
are favored due to length normalization.

An example of a tree-text query which works poorly is \texttt{computer monitor}. The top hits
are ``System Monitoring'' and ``LOBSTER'' (an internet traffic monitoring infrastructure)---probably
not what the user is looking for. The problem here is that the articles are retrieved based on their
use of the keyword ``monitor'', but there is no semantic analysis to distinguish the two different
meanings of the term.

\subsection{Phrase queries}

An example of a phrase query that was not ranked well is \texttt{``computer science''}---
the article entitled ``Computer science'' was ranked sixth, behind less relevant articles
such as ``Computer Science Tripos'' and ``e-Science''. In large part, this is likely because
we fail to treat article titles differently from the articles themselves. The query matches
an article title verbatim, but our search engine does not distinguish title from text.

A much more precise and less widely used term serves as a better phrase query. For instance,
the phrase query \texttt{extracellular matrix} indeed returns the few articles on biology
in the collection which refer to the matrix of proteins surrounding the cell.

\subsection{Boolean queries}

The boolean query \texttt{java AND garbage} correctly identifies articles about garbage collection,
and Java's particular mark-sweep implementation of garbage collection!

The query \texttt{memory AND (Neuroscience OR brain)} correctly identified neuroscience articles
rather than articles about computer memory.

The query \texttt{underwater AND machine} produces relevant hits---for example, an article
about the underwater research vessel SPURV. However, it fails to produce perhaps the most
obvious result, that being the article ``Submarine''. Although a submarine is an underwater machine,
this article does not make heavy use of this terminology. (Instead it uses terms like ``watercraft''
and ``submersible''.)

\subsection{Wildcard queries}

I can never remember how to spell Dijkstra, so I used the wildcard query
\texttt{di*str}, and voila!, the top hit was ``Edsger W. Dijkstra''.

Wildcard queries might not do what they are expected to do because
of stemming. For instance, the query \texttt{yout*be*} does not find
the article ``YouTube'' as a hit. This is because the term ``youtube''
is stemmed to ``youtub'', which will not match the input query.

\subsection{Wildcard phrase queries}

Given the query \texttt{``recurs* ca*''}, our search engine returns
``Recursive call'' as the highest ranked result.

A wildcard phrase query whose individual wildcard terms match large sets
of terms from the lexicon will return imprecise results. It will also be
slow because every possibility in the cartesian product has be checked
using phrase query functionality. For example, trying to search for
``electronic notes'' using \texttt{``elec* no*''} is not a good idea.

\section{Extra Features}

We found that some extra features arose naturally from our design, so we went
ahead and implemented them.

\subsection{Unlimited wildcards}

Our code supports an unlimited number of wildcards in a given query term. We also
do not have the restriction that consecutive wildcards are illegal. Thus,
\texttt{recur*si*ve*poly*} is an example of a valid one-word wildcard query, and
\texttt{``binar** se*arch tre*''} is a valid wildcard phrase query.

\subsection{Boolean and free text wildcard queries}

Our querying program allows boolean wildcard queries such as \texttt{(re* AND purpl*) OR (blu* AND *gree*))}, 
and free text wildcard queries such as \texttt{re* gree* *blu*)}.

\end{document}
