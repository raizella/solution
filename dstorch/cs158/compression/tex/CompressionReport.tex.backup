\documentclass[11pt]{article}

\usepackage{fullpage}

\parskip 7pt
\parindent 0pt

\title{Compression Contest}
\author{David Storch and Matt Mahoney \\
	logins: dstorch, mjmahone}
\date{4 March 2011}

\begin{document}

\maketitle

\section{Question 1}

We used the same strategy to compress both the positional and non-positional indexes.

The first step in the compression was converting the positional indices to relative indices, storing
the differences between consecutive values rather than the values themselves.
This was done for both the docIDs and the positional indices, i.e. 10:0,5,20 40:8,32 became
10:0,5,15 30:8,24.

The next step in the compression was, for each line, mapping each character to a half-byte, or nibble. As we only needed to
take care of 15 different characters, we made each digit (0-9) equivalent to it's corresponding half-byte
(i.e. 0 is 0000, 9 is  1001), with five special characters: newline maps to 1010, 
space to 1011, colon to 1100 and comma to 1101. 
We also reserved 1110 for an end-of-line blank-space (as it was possible the line had an odd number of characters in it, and we required each line to be a whole number of bytes).

The final step is to create a mini-index, which is compressed in the same way as 
the rest of the index file, that says which byte begins each line. Then, the first two bites of the compressed file are written saying the size of this index, in bytes, the mini-index is written, and then the compressed full-index is written.

This technique is very simple, but will only work for a limited character set.

\section{Question 2}

Our compressed non-positionally indexed file is 17,938,805 bytes, which is 25.7 \% 
the size of the original file while our compressed positionally-indexed file is
69,911,298 bytes, which is 35.6 \% the size of the original file. Relative to other
compressors, it appears that our positionally-indexed compressed did a better job, even
though in absolute terms the non-positionally indexed compressor compressed the file
to a smaller size.

\section{Question 3}

\centering
\begin{tabular}{|c|c|c||c|c|}
\hline
\textbf{compressor} & \textbf{non-positional size} & \textbf{positional size}
& \textbf{non-positional ratio} & \textbf{positional ratio}\\
\hline
gzip & 27,262,211 & 88,810,021 & 0.3910 & 0.4530 \\
bzip2 & 17,122,370 & 74,423,056 & 0.2456 & 0.3796 \\
7z & 13,568,351 & 69,788,951 & 0.1946 & 0.3560 \\
ours & 17,938,805 & 69,911,298 & 0.2573 & 0.3566 \\
uncompressed & 69,726,288 & 196,048,431 & 1 & 1\\
\hline
\end{tabular}

\flushleft
We did pretty similarly in our positional index to 7z, and better by a good amount than
bzip2 and gzip. With your non-positional index, however, we only did better than gzip.
That being said, we couldn't use these compressors as part of an inverted index, because
we can't query these files, as they can adjust their compression format for different
sections fo the file, and aren't storing an index of which line begins where.

\section{Question 4}

One possible improvement would have been to encode numbers as binary numbers, 
rather than using a nibble for each digit. If the file contained lots of large
numbers, this would have been a significant improvement.
This would have made decompression a bit more challenging, though, 
because we would have needed a way to deliminate
when a grouping of bytes was a number vs. a special character. 
In addition, you can do significantly better using a bit code than a byte code,
but this would require bit-packing, which is much mroe complicated to understand
given that standard machines are byte addressable.

\end{document}
